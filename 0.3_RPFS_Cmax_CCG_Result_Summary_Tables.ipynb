{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RPFS Problem GRASP x C&CG (Cmax objective) - Tables and Graphs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before running this notebook, please run notebooks 0.1 and 0.2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os, fnmatch\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.filterwarnings('ignore', category=DeprecationWarning)\n",
    "import glob\n",
    "import seaborn as sns\n",
    "import gzip\n",
    "import matplotlib.style as style\n",
    "from matplotlib.path import Path\n",
    "from matplotlib.patches import BoxStyle\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "if sys.version_info[0] < 3: \n",
    "    from StringIO import StringIO\n",
    "else:\n",
    "    from io import StringIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linestyle_tuple = [\n",
    "     ('dotted',                (0, (1, 1))),\n",
    "     ('dashed',                (0, (5, 5))),\n",
    "     ('densely dashed',        (0, (5, 1))),\n",
    "     ('dashdotdotted',         (0, (3, 5, 1, 5, 1, 5))),\n",
    "     ('densely dashdotdotted', (0, (3, 1, 1, 1, 1, 1))),\n",
    "\n",
    "     ('dashdotted',            (0, (3, 5, 1, 5))),\n",
    "     ('densely dashdotted',    (0, (3, 1, 1, 1))),\n",
    "     \n",
    "     ('loosely dashed',        (0, (5, 10))),\n",
    "     ('loosely dashdotted',    (0, (3, 10, 1, 10))),\n",
    "     \n",
    "\n",
    "     ('loosely dashdotdotted', (0, (3, 10, 1, 10, 1, 10))),\n",
    "     ('densely dotted',        (0, (1, 1))),\n",
    "     ('loosely dotted',        (0, (1, 10)))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/51483901/is-there-a-way-to-extend-the-solid-color-background-to-the-full-width-of-the-pag\n",
    "class ExtendedTextBox(BoxStyle._Base):\n",
    "    \"\"\"\n",
    "    An Extended Text Box that expands to the axes limits \n",
    "                        if set in the middle of the axes\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, pad=0.3, width=500.):\n",
    "        \"\"\"\n",
    "        width: \n",
    "            width of the textbox. \n",
    "            Use `ax.get_window_extent().width` \n",
    "                   to get the width of the axes.\n",
    "        pad: \n",
    "            amount of padding (in vertical direction only)\n",
    "        \"\"\"\n",
    "        self.width=width\n",
    "        self.pad = pad\n",
    "        super(ExtendedTextBox, self).__init__()\n",
    "\n",
    "    def transmute(self, x0, y0, width, height, mutation_size):\n",
    "        \"\"\"\n",
    "        x0 and y0 are the lower left corner of original text box\n",
    "        They are set automatically by matplotlib\n",
    "        \"\"\"\n",
    "        # padding\n",
    "        pad = mutation_size * self.pad\n",
    "\n",
    "        # we add the padding only to the box height\n",
    "        height = height + 2.*pad\n",
    "        # boundary of the padded box\n",
    "        y0 = y0 - pad\n",
    "        y1 = y0 + height\n",
    "        _x0 = x0\n",
    "        x0 = _x0 +width /2. - self.width/2.\n",
    "        x1 = _x0 +width /2. + self.width/2.\n",
    "\n",
    "        cp = [(x0, y0),\n",
    "              (x1, y0), (x1, y1), (x0, y1),\n",
    "              (x0, y0)]\n",
    "\n",
    "        com = [Path.MOVETO,\n",
    "               Path.LINETO, Path.LINETO, Path.LINETO,\n",
    "               Path.CLOSEPOLY]\n",
    "\n",
    "        path = Path(cp, com)\n",
    "\n",
    "        return path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List files in the result folder "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultfolder = os.path.join(os.getcwd(), 'results', 'consolidated')\n",
    "rpfs_ccg_file = os.path.join(resultfolder, 'RPFS_Cmax_CCG_all_results.pkl.gz')\n",
    "rpfs_grasp_file = os.path.join(resultfolder, 'RPFS_Cmax_GRASP_all_results.pkl.gz')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the output folder "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputfolder = os.path.join(os.getcwd(), 'results', 'consolidated')\n",
    "outputfolder_graph = os.path.join(os.getcwd(), 'results', 'consolidated', 'graphs')\n",
    "outputfolder_table = os.path.join(os.getcwd(), 'results', 'consolidated', 'tables')\n",
    "if not os.path.exists(outputfolder_graph):\n",
    "    os.makedirs(outputfolder_graph)\n",
    "if not os.path.exists(outputfolder_table):\n",
    "    os.makedirs(outputfolder_table)\n",
    "#print('Saving files on folder: ' + outputfolder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process consolidated result files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rpfs_ccg = pd.read_pickle(rpfs_ccg_file)  # Robust PFSP Budget solutions only\n",
    "df_rpfs_ccg.drop(columns=['executionId'], inplace=True)\n",
    "df_rpfs_ccg = df_rpfs_ccg.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rpfs_grasp = pd.read_pickle(rpfs_grasp_file)  # Robust PFSP Budget solutions only\n",
    "df_rpfs_grasp.drop(columns=['execution_id'], inplace=True)\n",
    "df_rpfs_grasp = df_rpfs_grasp.reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Robust dataframe: calculating new fields.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rpfs_ccg['optimal'] = df_rpfs_ccg['is_optimal'] & df_rpfs_ccg['validated'] & (df_rpfs_ccg['gap'] <= 1e-5)\n",
    "df_rpfs_ccg['time_limit'] = 7200.0\n",
    "df_rpfs_ccg['time_limit_2'] = 7200.0 * 2\n",
    "df_rpfs_ccg['mp_total_time'] = (df_rpfs_ccg['n'] < 15).astype(int) * np.minimum(df_rpfs_ccg['mp_total_time'], df_rpfs_ccg['time_limit']) + (df_rpfs_ccg['n'] >= 15).astype(int) * np.minimum(df_rpfs_ccg['mp_total_time'], df_rpfs_ccg['time_limit_2'])\n",
    "df_rpfs_ccg['time'] = df_rpfs_ccg['mp_total_time'] + df_rpfs_ccg['sp_total_time']\n",
    "df_rpfs_ccg['gap'] = df_rpfs_ccg['gap'] * 100.0\n",
    "df_rpfs_ccg['RobCost_worstcase'] = df_rpfs_ccg['cmax_validation']\n",
    "df_rpfs_ccg = df_rpfs_ccg.rename(columns={\"budget_Gamma\": \"RobCost_Gamma\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rpfs_grasp['time'] = df_rpfs_grasp['time_spent']\n",
    "df_rpfs_grasp['RobCost_worstcase'] = df_rpfs_grasp['solution_value']\n",
    "df_rpfs_grasp['RobCost_Gamma'] = df_rpfs_grasp['Gamma']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rpfs_ccg.tail(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rpfs_ccg.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking the Robust PFSP Budget C&CG solutions dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rpfs_ccg.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking the Robust PFSP Budget GRASP solutions dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rpfs_grasp.head(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Pre-process C&CG results dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replace model names with the name used in table presentation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rpfs_ccg['model'].replace({'wagner': 'Wagner', 'wilson': 'Wilson'}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtain list of C&CG models, instance types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_list = df_rpfs_ccg['model'].unique().tolist()\n",
    "instance_type_list = df_rpfs_ccg['instance_type'].unique().tolist()\n",
    "print(model_list)\n",
    "print(instance_type_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add a new column containing the instance size as string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_temp = df_rpfs_ccg\n",
    "(df_temp['n'].astype(str) + 'x' + df_temp['m'].astype(str)).unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rpfs_ccg.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_temp = df_rpfs_ccg\n",
    "df_temp['instance_size'] = df_temp['n'].astype(str) + 'x' + df_temp['m'].astype(str)\n",
    "df_rpfs_ccg = df_temp.set_index(['model', 'n', 'm', 'alpha_str', 'seq', 'RobCost_Gamma', 'instance_type'])\n",
    "df_rpfs_ccg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Treating errors in the `gap` column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rpfs_ccg['gap'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_check = df_rpfs_ccg.reset_index()[['model', 'n', 'm', 'alpha_str', 'seq', 'RobCost_Gamma', 'instance_name', 'gap', 'cmax', 'RobCost_worstcase', 'lb']]\n",
    "df_check[(df_check['gap'] < -1e-5)].to_csv(os.path.join(os.getcwd(), 'results', 'negative_gap_list.csv'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rpfs_ccg['gap'] = df_rpfs_ccg['gap'].apply(lambda x: np.maximum(x, 0.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rpfs_ccg['gap'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove alpha = R400 instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rpfs_ccg = df_rpfs_ccg.reset_index()\n",
    "len(df_rpfs_ccg[df_rpfs_ccg['alpha_str'] == 'R400'].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_rpfs_ccg = df_rpfs_ccg[df_rpfs_ccg['alpha_str'] != 'R400']\n",
    "len(df_rpfs_ccg.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove case study (non-ying) instances "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "df_ = df_rpfs_ccg.reset_index()\n",
    "display(len(df_[df_['seq'] == 'ce'].index))\n",
    "df_rpfs_ccg.reset_index(inplace=True)\n",
    "display(len(df_rpfs_ccg.index))\n",
    "df_rpfs = df_rpfs[df_rpfs['seq'] != 'ce']\n",
    "key_list = ['model', 'n', 'm', 'alpha', 'seq', 'RobCost_Gamma', 'instance_type']\n",
    "df_rpfs.set_index(key_list, verify_integrity=True, inplace=True)\n",
    "display(len(df_rpfs.index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rpfs_ccg.reset_index()['seq'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table 1. C&CG Performance given all instances \n",
    "\n",
    "Model-wise Robust PFSP C&CG performance comparison, given all instances.\n",
    "\n",
    "* % Best Performance is the percentage of instances solved to optimality where the model achieved shorter execution time, when compared to the other models; \n",
    "\n",
    "* % Solved contains the percentage of instances solved within the time limit; \n",
    "\n",
    "* % Solved < n x m > represents the percentage of solved instances of size n x m; \n",
    "\n",
    "* Avg. % Gap is the average percentage gap of solutions from instances not solved to optimality; \n",
    "\n",
    "* Median time is the median execution time, in seconds; \n",
    "\n",
    "* Median iterations is the median of the number of iterations performed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_perc_best_performance(df, model):\n",
    "    df_model = df.reset_index()\n",
    "    df_model = df_model[df_model['model'] == model]\n",
    "    df_model = df_model[df_model['optimal'] == True]    \n",
    "    df_model = df_model.set_index(['n', 'm', 'alpha', 'seq', 'RobCost_Gamma', 'instance_type'])\n",
    "    if len(df_model.index) == 0:\n",
    "        return np.nan\n",
    "    \n",
    "    df_others = df.reset_index()\n",
    "    df_others = df_others[df_others['model'] != model]\n",
    "    df_others = df_others[df_others['optimal'] == True] \n",
    "    group_columns = ['n', 'm', 'alpha', 'seq', 'RobCost_Gamma', 'instance_type']\n",
    "    df_best_performance = df_others[group_columns + ['time']].groupby(by=group_columns).min()['time']\n",
    "    df_best_performance = df_best_performance.to_frame()\n",
    "    if len(df_best_performance.index) == 0:\n",
    "        return np.nan\n",
    "    \n",
    "    df_compare = df_best_performance.join(df_model, how='inner', \n",
    "                                                     on=group_columns,\n",
    "                                                     lsuffix='_best')\n",
    "    df_compare['time_wins'] = (df_compare['time'] < df_compare['time_best']).astype(int)\n",
    "    return np.round(100.0 * df_compare['time_wins'].sum() / len(df_compare.index), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_perc_solved(df, model, instance_type = None, instance_size = None, alpha = None):\n",
    "    df_model = df.reset_index()\n",
    "    if model is not None:\n",
    "        df_model = df_model[df_model['model'] == model]\n",
    "    df_ = df_model\n",
    "    if instance_type is not None:\n",
    "        df_ = df_[df_['instance_type'] == instance_type]\n",
    "    if instance_size is not None:\n",
    "        df_ = df_[df_['instance_size'] == instance_size]\n",
    "    if alpha is not None:\n",
    "        df_ = df_[df_['alpha'] == alpha]\n",
    "    if len(df_.index) > 0:\n",
    "        return np.round(100.0 * len(df_[(df_['optimal'] == True)].index) / len(df_.index), 2)\n",
    "    else:\n",
    "        return np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Avg. % Gap is the average percentage gap of solutions from instances not solved to optimality\n",
    "def calculate_avg_perc_gap(df, model):\n",
    "    df_model = df.reset_index()\n",
    "    if model is not None:\n",
    "        df_model = df_model[df_model['model'] == model]\n",
    "    df_model = df_model[df_model['optimal'] == False]\n",
    "    if df_model['gap'].mean() >= 1e-2:\n",
    "        return np.round(df_model['gap'].mean(), 2)\n",
    "    else:\n",
    "        return df_model['gap'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_median_time(df, model, time_col_name):\n",
    "    df_model = df.reset_index()\n",
    "    if model is not None:\n",
    "        df_model = df_model[df_model['model'] == model]\n",
    "    return np.round(df_model[time_col_name].median(), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_avg_time(df, model, time_col_name):\n",
    "    df_model = df.reset_index()\n",
    "    if model is not None:\n",
    "        df_model = df_model[df_model['model'] == model]\n",
    "    return np.round(df_model[time_col_name].mean(), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_std(df, model, col_name):\n",
    "    df_model = df.reset_index()\n",
    "    if model is not None:\n",
    "        df_model = df_model[df_model['model'] == model]\n",
    "    if col_name == 'gap':\n",
    "        df_model = df_model[df_model['optimal'] == False]\n",
    "    if df_model[col_name].std() >= 1e-2:\n",
    "        return np.round(df_model[col_name].std(), 2)\n",
    "    else:\n",
    "        return df_model[col_name].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.stats\n",
    "\n",
    "def mean_confidence_interval(df, model, col_name, confidence=0.95):\n",
    "    df_model = df.reset_index()\n",
    "    if model is not None:\n",
    "        df_model = df_model[df_model['model'] == model]\n",
    "    if col_name == 'gap':\n",
    "        df_model = df_model[df_model['optimal'] == False]\n",
    "    data = df_model[col_name]\n",
    "    a = 1.0 * np.array(data)\n",
    "    n = len(a)\n",
    "    m, se = np.mean(a), scipy.stats.sem(a)\n",
    "    h = se * scipy.stats.t.ppf((1 + confidence) / 2., n-1)\n",
    "    lb = np.round(m-h, 2)\n",
    "    ub = np.round(m+h, 2)\n",
    "    if np.isnan(lb) or np.isnan(ub):\n",
    "        return '-'\n",
    "    return '[{}, {}]'.format(lb, ub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_median_iterations(df, model):\n",
    "    df_model = df.reset_index()\n",
    "    if model is not None:\n",
    "        df_model = df_model[df_model['model'] == model]\n",
    "    return np.round(df_model['iterations'].median(), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_avg_iterations(df, model):\n",
    "    df_model = df.reset_index()\n",
    "    if model is not None:\n",
    "        df_model = df_model[df_model['model'] == model]\n",
    "    return np.round(df_model['iterations'].mean(), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_num_instances(df, model):\n",
    "    df_model = df.reset_index()\n",
    "    if model is not None:\n",
    "        df_model = df_model[df_model['model'] == model]\n",
    "    return len(df_model.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1. All instances together, performance summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_stats = dict()\n",
    "df_base = df_rpfs_ccg\n",
    "exclude_model_list = []\n",
    "df_base = df_base[~(df_base['model'].isin(exclude_model_list))]\n",
    "model_list_reduced = [_ for _ in model_list if _ not in exclude_model_list]\n",
    "for model in model_list_reduced:\n",
    "    model_stats[model] = dict()\n",
    "    model_stats[model]['% Best Performance'] = calculate_perc_best_performance(df_base, model)\n",
    "    model_stats[model]['% Solved'] = calculate_perc_solved(df_base, model)  # given all instances\n",
    "    for instance_type in instance_type_list:  # group by instance type and size\n",
    "        df_itype = df_base\n",
    "        df_itype = df_itype[(df_itype['instance_type'] == instance_type)]\n",
    "        instance_size_list = df_itype['instance_size'].unique().tolist()\n",
    "        for instance_size in instance_size_list:\n",
    "            model_stats[model]['% Solved '+ instance_size] = calculate_perc_solved(df_base, model, instance_type, instance_size)\n",
    "    model_stats[model]['Avg. % gap'] = calculate_avg_perc_gap(df_base, model)\n",
    "    model_stats[model]['Median time'] = calculate_median_time(df_base, model, 'time')\n",
    "    model_stats[model]['Median time MP'] = calculate_median_time(df_base, model, 'mp_total_time')\n",
    "    model_stats[model]['Median time SP'] = calculate_median_time(df_base, model, 'sp_total_time')\n",
    "    model_stats[model]['Median iterations'] = calculate_median_iterations(df_base, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_stats_df = pd.DataFrame.from_dict(model_stats)\n",
    "model_stats_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export to Tableau, after melt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_stats_df.transpose().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "value_vars = model_stats_df.transpose().columns\n",
    "df_melt_model_stats_df = pd.melt(model_stats_df.transpose().reset_index(), id_vars=['index'], value_vars=value_vars)\n",
    "df_melt_model_stats_df['Model'] = df_melt_model_stats_df['index']\n",
    "df_melt_model_stats_df.to_excel(os.path.join(outputfolder_table, '1_ccg_cmax_model_stats_summary.xlsx'))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Save output table as HTML\n",
    "pd.set_option('colheader_justify', 'center')   # FOR TABLE <th>\n",
    "\n",
    "html_string = '''\n",
    "<html>\n",
    "  <head><title>HTML Pandas Dataframe with CSS</title></head>\n",
    "  <link rel=\"stylesheet\" type=\"text/css\" href=\"df_style.css\"/>\n",
    "  <body>\n",
    "    {table}\n",
    "  </body>\n",
    "</html>.\n",
    "'''\n",
    "\n",
    "# OUTPUT AN HTML FILE\n",
    "with open(os.path.join(outputfolder_table, 'model_stats.html'), 'w') as f:\n",
    "    f.write(html_string.format(table=model_stats_df.to_html(classes='mystyle')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table 2. Performance per instance group and model\n",
    "\n",
    "Model-wise Robust PFSP C&CG performance comparison, per instance group.\n",
    "\n",
    "* % Best Performance is the percentage of instances solved to optimality where the model achieved shorter execution time, when compared to the other models; \n",
    "\n",
    "* % Solved contains the percentage of instances solved within the time limit; \n",
    "\n",
    "* Avg. % Gap is the average percentage gap of solutions from instances not solved to optimality; \n",
    "\n",
    "* Avg. time and Std. dev. of time are the mean and standard deviation in solution time (s), respectively;\n",
    "\n",
    "* Avg. iterations and Std. dev. of iterations are the mean and standard deviation of the number of iterations performed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "per_instance_stats = dict()\n",
    "for instance_type in instance_type_list:  # group by instance type and size\n",
    "    df_base = df_rpfs_ccg.reset_index()\n",
    "    exclude_model_list = []\n",
    "    df_base = df_base[~(df_base['model'].isin(exclude_model_list))]\n",
    "    model_list_reduced = [_ for _ in model_list if _ not in exclude_model_list]\n",
    "    df_itype = df_base\n",
    "    df_itype = df_itype[(df_itype['instance_type'] == instance_type)]\n",
    "    instance_size_list = df_itype['instance_size'].unique().tolist()\n",
    "    for instance_size in instance_size_list:\n",
    "        df_instance = df_itype[df_itype['instance_size'] == instance_size]\n",
    "        for model in model_list_reduced:\n",
    "            per_instance_stats[(instance_type,instance_size,model)] = dict()\n",
    "            per_instance_stats[(instance_type,instance_size,model)]['% Best Performance'] = calculate_perc_best_performance(df_instance, model)\n",
    "            per_instance_stats[(instance_type,instance_size,model)]['% Solved'] = calculate_perc_solved(df_base, model, instance_type, instance_size)\n",
    "            per_instance_stats[(instance_type,instance_size,model)]['Avg. % gap'] = calculate_avg_perc_gap(df_instance, model)\n",
    "            per_instance_stats[(instance_type,instance_size,model)]['Std. dev. of % gap'] = calculate_std(df_instance, model, 'gap')\n",
    "            per_instance_stats[(instance_type,instance_size,model)]['95% CI of % gap'] = mean_confidence_interval(df_instance, model, 'gap')\n",
    "            per_instance_stats[(instance_type,instance_size,model)]['Avg. time'] = calculate_avg_time(df_instance, model, 'time')\n",
    "            per_instance_stats[(instance_type,instance_size,model)]['Std. dev. of time'] = calculate_std(df_instance, model, 'time')\n",
    "            per_instance_stats[(instance_type,instance_size,model)]['Avg. MP time'] = calculate_avg_time(df_instance, model, 'mp_total_time')\n",
    "            per_instance_stats[(instance_type,instance_size,model)]['Avg. SP time'] = calculate_avg_time(df_instance, model, 'sp_total_time')\n",
    "            per_instance_stats[(instance_type,instance_size,model)]['Avg. iterations'] = calculate_avg_iterations(df_instance, model)\n",
    "            per_instance_stats[(instance_type,instance_size,model)]['Std. dev. of iterations'] = calculate_std(df_instance, model, 'iterations')\n",
    "            per_instance_stats[(instance_type,instance_size,model)]['# instances solved'] = calculate_num_instances(df_instance, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/57606801/pandas-style-options-to-latex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "allowed_keys = [(x, y, z) for (x, y, z) in per_instance_stats.keys()]\n",
    "per_instance_stats1 = { your_key: per_instance_stats[your_key] for your_key in allowed_keys }\n",
    "df_table3a = pd.DataFrame.from_dict(per_instance_stats1)\n",
    "#df_table3a.columns = df_table3a.columns.droplevel()\n",
    "df_table3a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "allowed_keys = [(x, y, z) for (x, y, z) in per_instance_stats.keys() if (y in ['200x2', '10x4', '10x5', '15x5', '20x5'])]\n",
    "per_instance_stats1 = { your_key: per_instance_stats[your_key] for your_key in allowed_keys }\n",
    "df_table3a = pd.DataFrame.from_dict(per_instance_stats1)\n",
    "#df_table3a.columns = df_table3a.columns.droplevel()\n",
    "df_table3a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Export table to Tableau, after melt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_table3 = pd.DataFrame.from_dict(per_instance_stats)\n",
    "#df_table3.columns = df_table3.columns.droplevel()\n",
    "value_vars = df_table3.transpose().columns\n",
    "df_melt_table3 = pd.melt(df_table3.transpose().reset_index(), id_vars=['level_0', 'level_1', 'level_2'], value_vars=value_vars)\n",
    "df_melt_table3['Instance type'] = df_melt_table3['level_0']\n",
    "df_melt_table3['Instance size'] = df_melt_table3['level_1']\n",
    "df_melt_table3['Model'] = df_melt_table3['level_2']\n",
    "df_melt_table3.to_excel(os.path.join(outputfolder_table, '2_ccg_cmax_model_stats_per_instance.xlsx'))\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Save output table as HTML\n",
    "pd.set_option('colheader_justify', 'center')   # FOR TABLE <th>\n",
    "\n",
    "html_string = '''\n",
    "<html>\n",
    "  <head><title>HTML Pandas Dataframe with CSS</title></head>\n",
    "  <link rel=\"stylesheet\" type=\"text/css\" href=\"df_style.css\"/>\n",
    "  <body>\n",
    "    {table}\n",
    "  </body>\n",
    "</html>.\n",
    "'''\n",
    "\n",
    "# OUTPUT AN HTML FILE\n",
    "with open(os.path.join(outputfolder_table, 'instance_stats_1.html'), 'w') as f:\n",
    "    f.write(html_string.format(table=df_table3a.to_html(classes='mystyle')))\n",
    "with open(os.path.join(outputfolder_table, 'instance_stats_2.html'), 'w') as f:\n",
    "    f.write(html_string.format(table=df_table3b.to_html(classes='mystyle')))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "print(df_table3.to_latex(index=True)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table 3. Performance per Instance group, alpha and model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "per_instance_stats = dict()\n",
    "alpha_list = df_rpfs_ccg.reset_index()['alpha_str'].unique().tolist()\n",
    "for instance_type in instance_type_list:  # group by instance type and size\n",
    "    df_base = df_rpfs_ccg.reset_index()\n",
    "    exclude_model_list = []\n",
    "    df_base = df_base[~(df_base['model'].isin(exclude_model_list))]\n",
    "    model_list_reduced = [_ for _ in model_list if _ not in exclude_model_list]\n",
    "    df_itype = df_base\n",
    "    df_itype = df_itype[(df_itype['instance_type'] == instance_type)]\n",
    "    instance_size_list = df_itype['instance_size'].unique().tolist()\n",
    "    for instance_size in instance_size_list:\n",
    "        df_instance = df_itype[df_itype['instance_size'] == instance_size]\n",
    "        for alpha in alpha_list:\n",
    "            df_alpha = df_instance[df_instance['alpha_str'] == alpha]\n",
    "            for model in model_list_reduced:\n",
    "                key = (instance_type,instance_size,alpha,model)\n",
    "                per_instance_stats[key] = dict()\n",
    "                per_instance_stats[key]['% Best Performance'] = calculate_perc_best_performance(df_alpha, model)\n",
    "                per_instance_stats[key]['% Solved'] = calculate_perc_solved(df_base, model, instance_type, instance_size, alpha)\n",
    "                per_instance_stats[key]['Avg. % gap'] = calculate_avg_perc_gap(df_alpha, model)\n",
    "                per_instance_stats[key]['Std. dev. of % gap'] = calculate_std(df_alpha, model, 'gap')\n",
    "                per_instance_stats[key]['95% CI of % gap'] = mean_confidence_interval(df_alpha, model, 'gap')\n",
    "                per_instance_stats[key]['Avg. time'] = calculate_avg_time(df_alpha, model, 'time')\n",
    "                per_instance_stats[key]['Std. dev. of time'] = calculate_std(df_alpha, model, 'time')\n",
    "                per_instance_stats[key]['Avg. MP time'] = calculate_avg_time(df_alpha, model, 'mp_total_time')\n",
    "                per_instance_stats[key]['Avg. SP time'] = calculate_avg_time(df_alpha, model, 'sp_total_time')\n",
    "                per_instance_stats[key]['Avg. iterations'] = calculate_avg_iterations(df_alpha, model)\n",
    "                per_instance_stats[key]['Std. dev. of iterations'] = calculate_std(df_alpha, model, 'iterations')\n",
    "                per_instance_stats[key]['# instances solved'] = calculate_num_instances(df_alpha, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Export table to Tableau, after melt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_table_alpha = pd.DataFrame.from_dict(per_instance_stats)\n",
    "#df_table_alpha.columns = df_table_alpha.columns.droplevel()\n",
    "value_vars = df_table_alpha.transpose().columns\n",
    "#df_table_alpha.transpose()\n",
    "df_melt_table3 = pd.melt(df_table_alpha.transpose().reset_index(), id_vars=['level_0', 'level_1', 'level_2', 'level_3'], value_vars=value_vars)\n",
    "df_melt_table3['Instance type'] = df_melt_table3['level_0']\n",
    "df_melt_table3['Instance size'] = df_melt_table3['level_1']\n",
    "df_melt_table3['Alpha'] = df_melt_table3['level_2']\n",
    "df_melt_table3['Model'] = df_melt_table3['level_3']\n",
    "df_melt_table3\n",
    "df_melt_table3.to_excel(os.path.join(outputfolder_table, '3_ccg_cmax_model_stats_per_instance_and_alpha.xlsx'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Pre-process GRASP results dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add a new column containing the instance size as string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_temp = df_rpfs_grasp\n",
    "(df_temp['n'].astype(str) + 'x' + df_temp['m'].astype(str)).unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rpfs_grasp.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_temp = df_rpfs_grasp\n",
    "df_temp['instance_size'] = df_temp['n'].astype(str) + 'x' + df_temp['m'].astype(str)\n",
    "df_rpfs_grasp = df_temp.set_index(['n', 'm', 'alpha_str', 'seq', 'RobCost_Gamma', 'instance_type'])\n",
    "df_rpfs_grasp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove alpha = R400 instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rpfs_grasp = df_rpfs_grasp.reset_index()\n",
    "len(df_rpfs_grasp[df_rpfs_grasp['alpha_str'] == 'R400'].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_rpfs_grasp = df_rpfs_grasp[df_rpfs_grasp['alpha_str'] != 'R400']\n",
    "len(df_rpfs_grasp.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table 4. GRASP performance per instance group and model\n",
    "\n",
    "Model-wise Robust PFSP GRASP performance comparison, per instance group.\n",
    "\n",
    "* % Best Performance is the percentage of instances solved to optimality where the model achieved shorter execution time, when compared to the other models; \n",
    "\n",
    "* Avg. time and Std. dev. of time are the mean and standard deviation in solution time (s), respectively;\n",
    "\n",
    "* Avg. iterations and Std. dev. of iterations are the mean and standard deviation of the number of iterations performed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grasp_per_instance_stats = dict()\n",
    "for instance_type in instance_type_list:  # group by instance type and size\n",
    "    df_base = df_rpfs_grasp.reset_index()\n",
    "    df_itype = df_base\n",
    "    df_itype = df_itype[(df_itype['instance_type'] == instance_type)]\n",
    "    instance_size_list = df_itype['instance_size'].unique().tolist()\n",
    "    for instance_size in instance_size_list:\n",
    "        df_instance = df_itype[df_itype['instance_size'] == instance_size]\n",
    "        grasp_per_instance_stats[(instance_type,instance_size)] = dict()\n",
    "        model = None\n",
    "        \n",
    "        #grasp_per_instance_stats[(instance_type,instance_size)]['% Best Performance'] = calculate_perc_best_performance(df_instance, model)\n",
    "        #grasp_per_instance_stats[(instance_type,instance_size,model)]['% Solved'] = calculate_perc_solved(df_base, model, instance_type, instance_size)\n",
    "        #grasp_per_instance_stats[(instance_type,instance_size,model)]['Avg. % gap'] = calculate_avg_perc_gap(df_instance, model)\n",
    "        #grasp_per_instance_stats[(instance_type,instance_size,model)]['Std. dev. of % gap'] = calculate_std(df_instance, model, 'gap')\n",
    "        #grasp_per_instance_stats[(instance_type,instance_size,model)]['95% CI of % gap'] = mean_confidence_interval(df_instance, model, 'gap')\n",
    "        grasp_per_instance_stats[(instance_type,instance_size)]['Avg. time'] = calculate_avg_time(df_instance, model, 'time')\n",
    "        grasp_per_instance_stats[(instance_type,instance_size)]['Std. dev. of time'] = calculate_std(df_instance, model, 'time')\n",
    "        #grasp_per_instance_stats[(instance_type,instance_size,model)]['Avg. MP time'] = calculate_avg_time(df_instance, model, 'mp_total_time')\n",
    "        #grasp_per_instance_stats[(instance_type,instance_size,model)]['Avg. SP time'] = calculate_avg_time(df_instance, model, 'sp_total_time')\n",
    "        grasp_per_instance_stats[(instance_type,instance_size)]['Avg. iterations'] = calculate_avg_iterations(df_instance, model)\n",
    "        grasp_per_instance_stats[(instance_type,instance_size)]['Std. dev. of iterations'] = calculate_std(df_instance, model, 'iterations')\n",
    "        grasp_per_instance_stats[(instance_type,instance_size)]['# solutions obtained'] = calculate_num_instances(df_instance, model)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/57606801/pandas-style-options-to-latex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "allowed_keys = [(x, y) for (x, y) in grasp_per_instance_stats.keys()]\n",
    "per_instance_stats1 = { your_key: grasp_per_instance_stats[your_key] for your_key in allowed_keys }\n",
    "df_table4 = pd.DataFrame.from_dict(per_instance_stats1)\n",
    "#df_table4.columns = df_table3a.columns.droplevel()\n",
    "df_table4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table 5. C&CG vs. GRASP performance per instance group and model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vs_per_instance_stats = dict()\n",
    "for instance_type in instance_type_list:  # group by instance type and size\n",
    "    print('Instance type = ' + instance_type)\n",
    "    df_grasp = df_rpfs_grasp.reset_index()\n",
    "    df_grasp = df_grasp[(df_grasp['instance_type'] == instance_type)]\n",
    "    instance_size_list = df_grasp['instance_size'].unique().tolist()\n",
    "    \n",
    "    df_ccg = df_rpfs_ccg.reset_index()\n",
    "    df_ccg = df_ccg[(df_ccg['instance_type'] == instance_type)]\n",
    "    instance_size_list = df_ccg['instance_size'].unique().tolist()\n",
    "    \n",
    "    join_columns = ['instance_type', 'n', 'm', 'alpha_str', 'Gamma%']  # instance_name\n",
    "    for instance_size in instance_size_list:\n",
    "        print('Instance size = ' + instance_size)\n",
    "        vs_per_instance_stats[(instance_type,instance_size,model)] = dict()\n",
    "        df_grasp_instance = df_grasp[df_grasp['instance_size'] == instance_size]\n",
    "        for model in ['Wilson', 'Wagner']:\n",
    "            print('Model = ' + model)\n",
    "            df_ccg_instance = df_ccg[(df_ccg['instance_size'] == instance_size) & (df_ccg['model'] == model)]\n",
    "            df_instance = df_ccg_instance.merge(df_grasp_instance, on=join_columns, suffixes=('_ccg', '_grasp'))\n",
    "            df_instance['gap'] = df_instance['RobCost_worstcase_grasp'] - df_instance['RobCost_worstcase_ccg']\n",
    "            df_instance['gap%'] = df_instance['gap'] / df_instance['RobCost_worstcase_ccg']\n",
    "            print(df_instance[join_columns + ['gap', 'gap%']])\n",
    "            \n",
    "            break\n",
    "            #vs_per_instance_stats[(instance_type,instance_size,model)]['% Best Performance'] = calculate_perc_best_performance(df_instance, model)\n",
    "            #vs_per_instance_stats[(instance_type,instance_size,model)]['% Solved'] = calculate_perc_solved(df_base, model, instance_type, instance_size)\n",
    "            #vs_per_instance_stats[(instance_type,instance_size,model)]['Avg. % gap'] = calculate_avg_perc_gap(df_instance, model)\n",
    "            #vs_per_instance_stats[(instance_type,instance_size,model)]['Std. dev. of % gap'] = calculate_std(df_instance, model, 'gap')\n",
    "            #vs_per_instance_stats[(instance_type,instance_size,model)]['95% CI of % gap'] = mean_confidence_interval(df_instance, model, 'gap')\n",
    "            vs_per_instance_stats[(instance_type,instance_size,model)]['Avg. time'] = calculate_avg_time(df_instance, model, 'time')\n",
    "            vs_per_instance_stats[(instance_type,instance_size,model)]['Std. dev. of time'] = calculate_std(df_instance, model, 'time')\n",
    "            #vs_per_instance_stats[(instance_type,instance_size,model)]['Avg. MP time'] = calculate_avg_time(df_instance, model, 'mp_total_time')\n",
    "            #vs_per_instance_stats[(instance_type,instance_size,model)]['Avg. SP time'] = calculate_avg_time(df_instance, model, 'sp_total_time')\n",
    "            vs_per_instance_stats[(instance_type,instance_size,model)]['Avg. iterations'] = calculate_avg_iterations(df_instance, model)\n",
    "            vs_per_instance_stats[(instance_type,instance_size,model)]['Std. dev. of iterations'] = calculate_std(df_instance, model, 'iterations')\n",
    "            vs_per_instance_stats[(instance_type,instance_size,model)]['# solutions obtained'] = calculate_num_instances(df_instance, model)\n",
    "        break\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-rccp_cronos] *",
   "language": "python",
   "name": "conda-env-.conda-rccp_cronos-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
